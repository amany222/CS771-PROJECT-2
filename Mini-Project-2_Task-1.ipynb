{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1FbgS5AlRPXj6cOLCG7lVWk-fJPjmBLxY","authorship_tag":"ABX9TyMoHn4kC1NnIzswzlqM41AH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdNAxpPQYY_M","executionInfo":{"status":"ok","timestamp":1731385263484,"user_tz":-330,"elapsed":17830,"user":{"displayName":"Varsha Pillai","userId":"02286057017890317984"}},"outputId":"e559ede5-5b79-4eac-935d-12d14f92e3b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","import warnings\n","warnings.filterwarnings('ignore', category=UserWarning, message=\".*torch.tensor.*\")\n","warnings.filterwarnings('ignore', category=FutureWarning, message=\".*torch.load.*\")\n","\n","# Define a multi-layer perceptron (MLP) for feature extraction\n","class MLPFeatureExtractor(nn.Module):\n","    def __init__(self, input_size, hidden_size, feature_dim):\n","        super(MLPFeatureExtractor, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n","        self.fc2 = nn.Linear(hidden_size, feature_dim)  # Second fully connected layer\n","        self.relu = nn.ReLU()  # Activation function\n","\n","    def forward(self, x):\n","        x = self.relu(self.fc1(x))  # Apply ReLU to the output of the first layer\n","        return self.fc2(x)  # Output of the second layer\n","\n","# Define the Lightweight Prototype (LwP) classifier\n","class LwPClassifier:\n","    def __init__(self, n_classes=10, feature_dim=128):\n","        self.n_classes = n_classes  # Number of classes\n","        self.prototypes = np.zeros((n_classes, feature_dim))  # Prototype for each class\n","        self.class_counts = np.zeros(n_classes)  # Count of samples per class\n","\n","    def partial_fit(self, X, y):\n","        \"\"\"\n","        Update the prototypes incrementally for each sample.\n","        \"\"\"\n","        for i in range(len(X)):\n","            class_label = y[i]\n","            self.class_counts[class_label] += 1  # Increment count for the class\n","            if self.class_counts[class_label] == 1:\n","                # If first sample for the class, initialize prototype\n","                self.prototypes[class_label] = X[i]\n","            else:\n","                # Update prototype using the running average\n","                self.prototypes[class_label] = (\n","                    self.prototypes[class_label] * (self.class_counts[class_label] - 1) + X[i]\n","                ) / self.class_counts[class_label]\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict the class of each sample by finding the nearest prototype.\n","        \"\"\"\n","        distances = np.linalg.norm(X[:, np.newaxis] - self.prototypes, axis=2)\n","        return np.argmin(distances, axis=1)  # Return the class with minimum distance\n","\n","# Load a dataset from the specified path\n","def load_dataset(path, is_train=True):\n","    data = torch.load(path)  # Load dataset from file\n","\n","    inputs = data['data']  # Input data\n","    labels = data.get('targets', None)  # Labels (optional for test data)\n","\n","    # Convert inputs and labels to tensors if they are not already\n","    if not isinstance(inputs, torch.Tensor):\n","        inputs = torch.tensor(inputs, dtype=torch.float32)\n","    if labels is not None and not isinstance(labels, torch.Tensor):\n","        labels = torch.tensor(labels, dtype=torch.long)\n","\n","    # Flatten inputs if they have more than two dimensions\n","    if len(inputs.shape) > 2:\n","        inputs = inputs.view(inputs.shape[0], -1)\n","\n","    return (inputs, labels) if is_train else (inputs, labels)\n","\n","# Train the feature extractor and classifier on the initial labeled dataset\n","def train_initial_model(feature_extractor, classifier, dataset, labels):\n","    feature_extractor.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        features = feature_extractor(dataset).numpy()  # Extract features\n","    classifier.partial_fit(features, labels.numpy())  # Train the classifier\n","\n","# Update the classifier with pseudo-labeled data\n","def pseudo_label_and_update(feature_extractor, classifier, dataset):\n","    feature_extractor.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        features = feature_extractor(dataset).numpy()  # Extract features\n","    pseudo_labels = classifier.predict(features)  # Generate pseudo-labels\n","    classifier.partial_fit(features, pseudo_labels)  # Update classifier with pseudo-labels\n","\n","# Evaluate the model's accuracy on the given dataset\n","def evaluate_model(feature_extractor, classifier, dataset, true_labels):\n","    feature_extractor.eval()  # Set model to evaluation mode\n","    with torch.no_grad():\n","        features = feature_extractor(dataset).numpy()  # Extract features\n","    predictions = classifier.predict(features)  # Predict labels\n","    accuracy = accuracy_score(true_labels.numpy(), predictions)  # Compute accuracy\n","    return accuracy\n","\n","# Perform continual learning across multiple datasets\n","def continual_learning(base_path_train, base_path_eval, input_size, hidden_size, feature_dim, n_classes):\n","    accuracies = []  # List to store accuracy results for each task\n","\n","    # Initialize the feature extractor and classifier\n","    feature_extractor = MLPFeatureExtractor(input_size, hidden_size, feature_dim)\n","    classifier = LwPClassifier(n_classes=n_classes, feature_dim=feature_dim)\n","\n","    # Loop through all datasets for continual learning\n","    for i in range(1, 11):\n","        # Load the training dataset for the current task\n","        train_dataset_path = f\"{base_path_train}/{i}_train_data.tar.pth\"\n","        train_data, train_labels = load_dataset(train_dataset_path, is_train=True)\n","\n","        # Load the evaluation dataset for the current task\n","        eval_dataset_path = f\"{base_path_eval}/{i}_eval_data.tar.pth\"\n","        eval_data, eval_labels = load_dataset(eval_dataset_path, is_train=False)\n","\n","        if i == 1:\n","            # Train on the first dataset with labels\n","            train_initial_model(feature_extractor, classifier, train_data, train_labels)\n","        else:\n","            # Pseudo-label and update for subsequent datasets\n","            pseudo_label_and_update(feature_extractor, classifier, train_data)\n","\n","        # Evaluate the model on all tasks seen so far\n","        model_accuracies = []\n","        for j in range(1, i + 1):\n","            eval_dataset_path_j = f\"{base_path_eval}/{j}_eval_data.tar.pth\"\n","            eval_data_j, eval_labels_j = load_dataset(eval_dataset_path_j, is_train=False)\n","\n","            eval_acc = evaluate_model(feature_extractor, classifier, eval_data_j, eval_labels_j)\n","            model_accuracies.append(eval_acc)\n","\n","        # Fill the rest of the row with `None` for tasks not yet encountered\n","        while len(model_accuracies) < 10:\n","            model_accuracies.append(None)\n","\n","        accuracies.append(model_accuracies)  # Add the accuracies for this task\n","\n","    return accuracies  # Return the accuracy matrix\n","\n","# Paths and model parameters\n","base_path_train = \"dataset/part_one_dataset/train_data\"  # Path to training datasets\n","base_path_eval = \"dataset/part_one_dataset/eval_data\"  # Path to evaluation datasets\n","\n","input_size = 3072  # Example: CIFAR-10 image flattened size\n","hidden_size = 256  # Size of the hidden layer in the MLP\n","feature_dim = 128  # Dimensionality of extracted features\n","n_classes = 10  # Number of classes\n","\n","# Perform continual learning and get accuracy matrix\n","accuracy_matrix = continual_learning(base_path_train, base_path_eval, input_size, hidden_size, feature_dim, n_classes)\n","\n","# Print the accuracy matrix\n","print(\"\\nAccuracy matrix:\")\n","for row in accuracy_matrix:\n","    print(\" | \".join(f\"{acc:.4f}\" if acc is not None else \"  N/A \" for acc in row))\n"],"metadata":{"id":"ml4es2VkMt3e","executionInfo":{"status":"ok","timestamp":1732644984297,"user_tz":-330,"elapsed":12668,"user":{"displayName":"Varsha Pillai","userId":"02286057017890317984"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f05b8c40-be49-407c-dd33-e0440e440a16"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Accuracy matrix:\n","0.2564 |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A \n","0.2500 | 0.2436 |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A \n","0.2492 | 0.2428 | 0.2296 |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A \n","0.2440 | 0.2396 | 0.2300 | 0.2380 |   N/A  |   N/A  |   N/A  |   N/A  |   N/A  |   N/A \n","0.2412 | 0.2348 | 0.2244 | 0.2356 | 0.2644 |   N/A  |   N/A  |   N/A  |   N/A  |   N/A \n","0.2412 | 0.2336 | 0.2212 | 0.2348 | 0.2620 | 0.2232 |   N/A  |   N/A  |   N/A  |   N/A \n","0.2396 | 0.2332 | 0.2196 | 0.2320 | 0.2592 | 0.2240 | 0.2296 |   N/A  |   N/A  |   N/A \n","0.2388 | 0.2288 | 0.2180 | 0.2304 | 0.2564 | 0.2220 | 0.2276 | 0.2396 |   N/A  |   N/A \n","0.2380 | 0.2284 | 0.2168 | 0.2280 | 0.2548 | 0.2188 | 0.2260 | 0.2380 | 0.2380 |   N/A \n","0.2360 | 0.2288 | 0.2168 | 0.2272 | 0.2520 | 0.2164 | 0.2264 | 0.2388 | 0.2356 | 0.2320\n"]}]}]}